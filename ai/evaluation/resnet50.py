# -*- coding: utf-8 -*-
"""ResNet50

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OiDq580D75Suw6-BsReABvgheobSslGW
"""

# ============================================
# 1) Google Drive Ïó∞Í≤∞
# ============================================
from google.colab import drive
drive.mount('/content/drive')

# ============================================
# 2) ÎùºÏù¥Î∏åÎü¨Î¶¨ Import
# ============================================
import os
from glob import glob
import re

# ============================================
# 3) Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏÑ§Ï†ï (Train / Val Î∂ÑÎ¶¨Îê®)
# ============================================
TRAIN_DIR = "/content/drive/MyDrive/ÏµúÏ¢Ödataset/train/images"
VAL_DIR   = "/content/drive/MyDrive/data/eval_700"

IMG_EXT = ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.webp"]

def load_paths(directory):
    paths = []
    for ext in IMG_EXT:
        paths.extend(glob(os.path.join(directory, ext)))
    return paths

train_paths = load_paths(TRAIN_DIR)
val_paths   = load_paths(VAL_DIR)

print("Train Ïù¥ÎØ∏ÏßÄ Ïàò:", len(train_paths))
print("Val Ïù¥ÎØ∏ÏßÄ Ïàò:", len(val_paths))

# ============================================
# 4) ÌååÏùºÎ™Ö ‚Üí ÎùºÎ≤®Î™Ö Ï∂îÏ∂ú
#    ÌòïÏãù: train_Í≤∞ÎßâÏóº_1.jpg ‚Üí "Í≤∞ÎßâÏóº"
# ============================================
def extract_label(filename):
    name = os.path.basename(filename)
    name = os.path.splitext(name)[0]   # train_Í≤∞ÎßâÏóº_1
    parts = name.split("_")

    # Í∏∞ÎåÄ ÌòïÌÉú: ["train", "Í≤∞ÎßâÏóº", "1"]
    if len(parts) >= 3:
        return parts[1]   # Îëê Î≤àÏß∏ Î∂ÄÎ∂ÑÏù¥ ÎùºÎ≤®Î™Ö

    return name           # fallback

# ============================================
# 5) ÎùºÎ≤® Î™©Î°ù ÏÉùÏÑ± ‚Üí ID Îß§Ìïë
# ============================================
all_labels = [extract_label(p) for p in train_paths + val_paths]
unique_labels = sorted(list(set(all_labels)))

print("ÌÅ¥ÎûòÏä§ Î™©Î°ù:", unique_labels)

num_classes = len(unique_labels)
label2id = {lbl: i for i, lbl in enumerate(unique_labels)}
id2label = {i: lbl for lbl, i in label2id.items()}

# ============================================
# 6) Dataset Ï†ïÏùò
# ============================================
from PIL import Image
from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
    def __init__(self, image_paths, transform=None):
        self.paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        img_path = self.paths[idx]
        img = Image.open(img_path).convert("RGB")

        label_name = extract_label(img_path)
        label = label2id[label_name]

        if self.transform:
            img = self.transform(img)

        return img, label

# ============================================
# 7) Transform Ï†ïÏùò
# ============================================
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ============================================
# 8) DataLoader ÏÉùÏÑ±
# ============================================
from torch.utils.data import DataLoader

train_dataset = CustomImageDataset(train_paths, transform=train_transform)
val_dataset   = CustomImageDataset(val_paths,   transform=val_transform)

BATCH_SIZE = 32

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)

print("DataLoader Ï§ÄÎπÑ ÏôÑÎ£å")

# ============================================
# 9) ResNet50 Î™®Îç∏ ÌïôÏäµ ÏΩîÎìú
# ============================================
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from tqdm.auto import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ResNet50 Î∂àÎü¨Ïò§Í∏∞
resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)
resnet = resnet.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(resnet.parameters(), lr=1e-4)

# ============================================
# 10) Train / Eval Ìï®Ïàò
# ============================================
def train_one_epoch(model, loader):
    model.train()
    total, correct, total_loss = 0, 0, 0

    for x, y in tqdm(loader):
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()

        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * x.size(0)
        correct += (pred.argmax(1) == y).sum().item()
        total += x.size(0)

    return total_loss / total, correct / total


@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    total, correct, total_loss = 0, 0, 0

    for x, y in tqdm(loader):
        x, y = x.to(device), y.to(device)
        pred = model(x)
        loss = criterion(pred, y)

        total_loss += loss.item() * x.size(0)
        correct += (pred.argmax(1) == y).sum().item()
        total += x.size(0)

    return total_loss / total, correct / total

# ============================================
# 11) ÌïôÏäµ ÏãúÏûë
# ============================================
EPOCHS = 5
best_acc = 0

for epoch in range(EPOCHS):
    tr_loss, tr_acc = train_one_epoch(resnet, train_loader)
    val_loss, val_acc = evaluate(resnet, val_loader)

    print(f"[{epoch+1}/{EPOCHS}] train_acc={tr_acc:.3f}, val_acc={val_acc:.3f}")

    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(resnet.state_dict(), "best_resnet50.pth")
        print("üî• Î™®Îç∏ Ï†ÄÏû•Îê®")

print("ÏµúÏ¢Ö ÏµúÍ≥† Ï†ïÌôïÎèÑ:", best_acc)

# ======================================================
# 12) Confusion Matrix + Metrics Í≥ÑÏÇ∞
# ======================================================
import numpy as np
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    precision_recall_fscore_support
)

@torch.no_grad()
def get_all_preds(model, loader):
    model.eval()
    preds = []
    labels = []
    for x, y in loader:
        x = x.to(device)
        logits = model(x)
        pred = logits.argmax(1).cpu().numpy()
        preds.extend(pred)
        labels.extend(y.numpy())
    return np.array(preds), np.array(labels)

# ---- ÏòàÏ∏°Í∞í ÏñªÍ∏∞ ----
preds, labels = get_all_preds(resnet, val_loader)

# ---- Summary Metrics Í≥ÑÏÇ∞ ----
accuracy = accuracy_score(labels, preds)

precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
    labels, preds, average="macro"
)
precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
    labels, preds, average="micro"
)
precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
    labels, preds, average="weighted"
)

# ---- Confusion Matrix Í≥ÑÏÇ∞ ----
cm = confusion_matrix(labels, preds)

# ======================================================
# 13) Excel Ï∂úÎ†• (Summary + Per-class + CM)
# ======================================================
import pandas as pd

save_path = "/content/drive/MyDrive/data/resnet50_results.xlsx"

# --------------------------
# Summary Metrics DataFrame
# --------------------------
summary_df = pd.DataFrame({
    "metric": [
        "accuracy",
        "coverage",
        "precision_macro", "recall_macro", "f1_macro",
        "precision_micro", "recall_micro", "f1_micro",
        "precision_weighted", "recall_weighted", "f1_weighted",
    ],
    "value": [
        accuracy,
        1.0,
        precision_macro, recall_macro, f1_macro,
        precision_micro, recall_micro, f1_micro,
        precision_weighted, recall_weighted, f1_weighted,
    ]
})

# ---------------------------
# Per-class Report DataFrame
# ---------------------------
prec, rec, f1, support = precision_recall_fscore_support(labels, preds)

per_class_df = pd.DataFrame({
    "class": unique_labels,
    "precision": prec,
    "recall": rec,
    "f1-score": f1,
    "support": support
})

overall_df = pd.DataFrame({
    "class": ["accuracy", "macro avg", "weighted avg"],
    "precision": [accuracy, precision_macro, precision_weighted],
    "recall":    [accuracy, recall_macro, recall_weighted],
    "f1-score":  [accuracy, f1_macro, f1_weighted],
    "support":   [0, len(labels), len(labels)]
})

per_class_df = pd.concat([per_class_df, overall_df], ignore_index=True)

# ---------------------------
# Confusion Matrix DataFrame
# ---------------------------
cm_df = pd.DataFrame(
    cm,
    index=[f"T:{lbl}" for lbl in unique_labels],
    columns=[f"P:{lbl}" for lbl in unique_labels]
)

# ---------------------------
# Ï†úÎ™©Í≥º Ìï®Íªò ExcelÏóê Í∏∞Î°ù
# ---------------------------
with pd.ExcelWriter(save_path, engine="openpyxl") as writer:

    # Summary Metrics
    summary_title = pd.DataFrame(["Summary Metrics"])
    summary_title.to_excel(writer, sheet_name="Summary Metrics", index=False, header=False)
    summary_df.to_excel(writer, sheet_name="Summary Metrics", index=False, startrow=2)

    # Per-class Report
    class_title = pd.DataFrame(["Per-class Report (precision / recall / f1-score / support)"])
    class_title.to_excel(writer, sheet_name="Per-class Report", index=False, header=False)
    per_class_df.to_excel(writer, sheet_name="Per-class Report", index=False, startrow=2)

    # Confusion Matrix
    cm_title = pd.DataFrame(["Confusion Matrix"])
    cm_title.to_excel(writer, sheet_name="Confusion Matrix", index=False, header=False)
    cm_df.to_excel(writer, sheet_name="Confusion Matrix", index=True, startrow=2)

print("üìÅ Excel ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å ‚Üí", save_path)