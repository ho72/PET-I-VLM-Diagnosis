# -*- coding: utf-8 -*-
"""CLIP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jgpHppibJcnYuW1QGyDc-NepQ_Y-mupw
"""

# ============================================
# 1) Google Drive ì—°ê²°
# ============================================
from google.colab import drive
drive.mount('/content/drive')

# ============================================
# 2) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================
import os
from glob import glob
import re
from PIL import Image

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ============================================
# 3) ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (Train / Val ë¶„ë¦¬ëœ êµ¬ì¡°)
# ============================================
TRAIN_DIR = "/content/drive/MyDrive/ìµœì¢…dataset/train/images"
VAL_DIR   = "/content/drive/MyDrive/data/eval_700"

IMG_EXT = ["*.jpg", "*.jpeg", "*.png", "*.bmp", "*.webp"]

def load_paths(directory):
    paths = []
    for ext in IMG_EXT:
        paths.extend(glob(os.path.join(directory, ext)))
    return paths

train_paths = load_paths(TRAIN_DIR)
val_paths   = load_paths(VAL_DIR)

print("Train ì´ë¯¸ì§€ ìˆ˜:", len(train_paths))
print("Val ì´ë¯¸ì§€ ìˆ˜:", len(val_paths))

# ============================================
# 4) íŒŒì¼ëª…ì—ì„œ ë¼ë²¨ ì¶”ì¶œ í•¨ìˆ˜
#    í˜•ì‹: train_ê²°ë§‰ì—¼_1.jpg â†’ "ê²°ë§‰ì—¼"
# ============================================
def extract_label(filename):
    name = os.path.basename(filename)
    name = os.path.splitext(name)[0]      # train_ê²°ë§‰ì—¼_1
    parts = name.split("_")               # ["train", "ê²°ë§‰ì—¼", "1"]

    if len(parts) >= 3:
        return parts[1]                   # ë¼ë²¨ëª…
    return name                           # fallback

# ============================================
# 5) ì „ì²´ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ë¼ë²¨ ID ë§¤í•‘
# ============================================
all_labels = [extract_label(p) for p in train_paths + val_paths]
unique_labels = sorted(list(set(all_labels)))
print("í´ë˜ìŠ¤ ëª©ë¡:", unique_labels)
num_classes = len(unique_labels)

label2id = {lbl: i for i, lbl in enumerate(unique_labels)}
id2label = {i: lbl for lbl, i in label2id.items()}

# ============================================
# 6) Custom Dataset ì •ì˜
# ============================================
class CustomImageDataset(Dataset):
    def __init__(self, image_paths, transform=None):
        self.paths = image_paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        img_path = self.paths[idx]

        img = Image.open(img_path).convert("RGB")
        label_name = extract_label(img_path)
        label = label2id[label_name]

        if self.transform:
            img = self.transform(img)

        return img, label

# ============================================
# 7) Transform ì •ì˜
# ============================================
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ============================================
# 8) Dataset / DataLoader êµ¬ì„±
# ============================================
train_dataset = CustomImageDataset(train_paths, transform=train_transform)
val_dataset   = CustomImageDataset(val_paths,   transform=val_transform)

BATCH_SIZE = 32

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)

print("DataLoader ì¤€ë¹„ ì™„ë£Œ")

# ============================================
# 9) CLIP ëª¨ë¸ ë¡œë“œ (Image Encoderë§Œ ì‚¬ìš©)
# ============================================
!pip install -q open_clip_torch

import open_clip

clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
    "ViT-B-32", pretrained="laion2b_s34b_b79k"
)
clip_model = clip_model.to(device)
clip_model.eval()

for p in clip_model.parameters():
    p.requires_grad = False  # freeze encoder

embed_dim = clip_model.visual.output_dim
print("CLIP Image Embed Dim:", embed_dim)

# ============================================
# 10) CLIPìš© Dataset ì¬ìƒì„± (clip_preprocess ì‚¬ìš©)
# ============================================
train_dataset_clip = CustomImageDataset(train_paths, transform=clip_preprocess)
val_dataset_clip   = CustomImageDataset(val_paths,   transform=clip_preprocess)

train_loader_clip = DataLoader(train_dataset_clip, batch_size=32, shuffle=True)
val_loader_clip   = DataLoader(val_dataset_clip,   batch_size=32, shuffle=False)

# ============================================
# 11) Linear Classifier ì •ì˜
# ============================================
classifier = nn.Linear(embed_dim, num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer_clip = optim.Adam(classifier.parameters(), lr=1e-3)

# ============================================
# 12) Feature Extraction í•¨ìˆ˜
# ============================================
def extract_features(imgs):
    with torch.no_grad():
        feats = clip_model.encode_image(imgs)
    return feats

# ============================================
# 13) CLIP Linear Probe í•™ìŠµ í•¨ìˆ˜
# ============================================
def train_clip_epoch():
    classifier.train()
    total, correct, total_loss = 0, 0, 0

    for x, y in tqdm(train_loader_clip):
        x, y = x.to(device), y.to(device)

        feats = extract_features(x)
        logits = classifier(feats)
        loss = criterion(logits, y)

        optimizer_clip.zero_grad()
        loss.backward()
        optimizer_clip.step()

        total_loss += loss.item() * x.size(0)
        correct += (logits.argmax(1) == y).sum().item()
        total += x.size(0)

    return total_loss / total, correct / total


@torch.no_grad()
def eval_clip_epoch():
    classifier.eval()
    total, correct, total_loss = 0, 0, 0

    for x, y in tqdm(val_loader_clip):
        x, y = x.to(device), y.to(device)

        feats = extract_features(x)
        logits = classifier(feats)
        loss = criterion(logits, y)

        total_loss += loss.item() * x.size(0)
        correct += (logits.argmax(1) == y).sum().item()
        total += x.size(0)

    return total_loss / total, correct / total

# ============================================
# 14) í•™ìŠµ ì‹¤í–‰
# ============================================
best_acc = 0

for epoch in range(5):
    tr_loss, tr_acc = train_clip_epoch()
    val_loss, val_acc = eval_clip_epoch()

    print(f"[Epoch {epoch+1}] train_acc={tr_acc:.3f}, val_acc={val_acc:.3f}")

    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(classifier.state_dict(), "best_clip_classifier.pth")
        print("ğŸ”¥ Best model saved!")

print("ìµœì¢… ìµœê³  ì •í™•ë„:", best_acc)

# ======================================================
# 16) ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
# ======================================================
import pandas as pd

save_path = "/content/drive/MyDrive/data/clip_results.xlsx"   # ì›í•˜ëŠ” ê²½ë¡œë¡œ ë³€ê²½

# ---- Summary Metrics ì €ì¥ ----
summary_df = pd.DataFrame({
    "metric": [
        "accuracy",
        "precision_macro", "recall_macro", "f1_macro",
        "precision_micro", "recall_micro", "f1_micro",
        "precision_weighted", "recall_weighted", "f1_weighted"
    ],
    "value": [
        accuracy,
        precision_macro, recall_macro, f1_macro,
        precision_micro, recall_micro, f1_micro,
        precision_weighted, recall_weighted, f1_weighted
    ]
})


# ---- Per-class Report ì €ì¥ ----
from sklearn.metrics import precision_recall_fscore_support

prec, rec, f1, support = precision_recall_fscore_support(labels, preds)

per_class_df = pd.DataFrame({
    "class": unique_labels,
    "precision": prec,
    "recall": rec,
    "f1-score": f1,
    "support": support
})


# ---- Confusion Matrix ì €ì¥ ----
cm_df = pd.DataFrame(
    cm,
    index=[f"T:{l}" for l in unique_labels],     # true label
    columns=[f"P:{l}" for l in unique_labels]    # predicted label
)


# ---- Excelë¡œ ì €ì¥ ----
with pd.ExcelWriter(save_path, engine="openpyxl") as writer:
    summary_df.to_excel(writer, sheet_name="Summary Metrics", index=False)
    per_class_df.to_excel(writer, sheet_name="Per-class Report", index=False)
    cm_df.to_excel(writer, sheet_name="Confusion Matrix")

print("ğŸ“ Excel ì €ì¥ ì™„ë£Œ:", save_path)